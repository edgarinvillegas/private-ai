{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 6 \"Differential Privacy for Deep Learning\" project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario description \n",
    "\n",
    "\n",
    "Training set: 60000 labeled images. This will be split in 20 slices of 3000 labeled images to generate the private models.\n",
    "Test set: 10000 images.\n",
    "The test set will have 2 purposes:\n",
    " - To help measure the training accuracy (using its labels for this)\n",
    " - To act as the \"public dataset\", by ignoring its labels. This is the main purpose of the testset\n",
    " \n",
    " Steps\n",
    "\n",
    "- 1) Train the 20 private models with the trainset\n",
    "- 2) Use the 20 partner models to predict on the local dataset (the testset), generating 20 labels for each of the datapoints\n",
    "- 3) Then, for each local data point (now with 20 labels), perform a maxcount query to get the most frequent label across the 20 labels. \n",
    "- 4) Add laplacian noise to the maxcounts to make this Differentially Private to a certain epsilon/delta constraint.\n",
    "- 5) Finally, we will retrain a new model on our local dataset which now has labels (the noised ones). This will be our final \"DP\" model.\n",
    "- 5) Do the PATE analysis to find the total epsilon used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project implementation\n",
    "\n",
    "This project will use the MNIST dataset, provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: ./data\n",
       "    Split: Test"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import torchvision.datasets as datasets\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "    #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "mnist_testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edgarin\\.conda\\envs\\pysyft-env\\lib\\site-packages\\torchvision\\datasets\\mnist.py:53: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n",
      "C:\\Users\\edgarin\\.conda\\envs\\pysyft-env\\lib\\site-packages\\torchvision\\datasets\\mnist.py:43: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "train_data = mnist_trainset.train_data\n",
    "train_targets = mnist_trainset.train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edgarin\\.conda\\envs\\pysyft-env\\lib\\site-packages\\torchvision\\datasets\\mnist.py:58: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "C:\\Users\\edgarin\\.conda\\envs\\pysyft-env\\lib\\site-packages\\torchvision\\datasets\\mnist.py:48: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "test_data = mnist_testset.test_data\n",
    "test_targets = mnist_testset.test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mnist_trainset.test_data.sum() == mnist_trainset.train_data.sum() True\n",
    "#mnist_testset.test_data.sum() == mnist_testset.train_data.sum() True\n",
    "#mnist_testset.test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 28, 28])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mnist_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "        # Dropout module with 0.2 drop probability\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Now with dropout\n",
    "        x = self.dropout(F.relu(self.fc1(x)))        \n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "\n",
    "        # output so no dropout here\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def predict_dataset(self, dataloader):\n",
    "        model = self\n",
    "        #predictions = list()\n",
    "        predictions = torch.tensor([]).long()\n",
    "        with torch.no_grad():\n",
    "            for images, _ in dataloader:                \n",
    "                log_ps = model(images)                \n",
    "                ps = torch.exp(log_ps)\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                prediction = top_class.view(top_class.shape[0])\n",
    "                #print('prediction.shape', prediction.shape)\n",
    "                predictions = torch.cat((predictions, prediction), 0)\n",
    "                #predictions.append(prediction)\n",
    "                \n",
    "        return predictions\n",
    "            \n",
    "    def train_and_test(self, trainloader, testloader, epochs = 1):\n",
    "        # Copied from Part 5 - Inference and Validation (Solution).ipynb\n",
    "        model = self\n",
    "        \n",
    "        criterion = nn.NLLLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "        # epochs = 30  #Only thing changed from original\n",
    "        steps = 0\n",
    "\n",
    "        train_losses, test_losses = [], []\n",
    "        for e in range(epochs):\n",
    "            print('Epoch {}'.format(e+1))\n",
    "            running_loss = 0\n",
    "            for images, labels in trainloader:\n",
    "                # images = images.view(images.shape[0], -1)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                log_ps = model(images)\n",
    "                loss = criterion(log_ps, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            else:\n",
    "                test_loss = 0\n",
    "                accuracy = 0\n",
    "                \n",
    "                # Turn off gradients for validation, saves memory and computations                \n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    for images, labels in testloader:\n",
    "                        # images = images.view(images.shape[0], -1)\n",
    "                        log_ps = model(images)\n",
    "                        test_loss += criterion(log_ps, labels)\n",
    "\n",
    "                        ps = torch.exp(log_ps)\n",
    "                        top_p, top_class = ps.topk(1, dim=1)\n",
    "                        equals = top_class == labels.view(*top_class.shape)\n",
    "                        accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "\n",
    "                model.train()\n",
    "\n",
    "                train_losses.append(running_loss/len(trainloader))\n",
    "                test_losses.append(test_loss/len(testloader))\n",
    "\n",
    "                print(\"Epoch: {}/{} \".format(e+1, epochs),\n",
    "                      \"Training Loss: {:.3f} \".format(running_loss/len(trainloader)),\n",
    "                      \"Test Loss: {:.3f} \".format(test_loss/len(testloader)),\n",
    "                      \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(mnist_trainset, batch_size=64, shuffle=False)\n",
    "testloader = torch.utils.data.DataLoader(mnist_testset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Epoch: 1/1  Training Loss: 0.329  Test Loss: 0.181  Test Accuracy: 0.947\n"
     ]
    }
   ],
   "source": [
    "## Test training a model\n",
    "model1 = Classifier()\n",
    "model1.train_and_test(trainloader, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import helper\n",
    "\n",
    "images, labels = next(iter(testloader))\n",
    "img = images[0] #.view(1, 784)\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logps = model1.forward(img)\n",
    "\n",
    "# Output of the network are logits, need to take softmax for probabilities\n",
    "ps = torch.exp(logps)\n",
    "helper.view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training slice  0 ...\n",
      "Epoch 1\n",
      "Epoch: 1/1  Training Loss: 1.153  Test Loss: 0.518  Test Accuracy: 0.851\n",
      "Training slice  1 ...\n",
      "Epoch 1\n",
      "Epoch: 1/1  Training Loss: 1.089  Test Loss: 0.487  Test Accuracy: 0.864\n",
      "Training slice  2 ...\n",
      "Epoch 1\n",
      "Epoch: 1/1  Training Loss: 1.179  Test Loss: 0.535  Test Accuracy: 0.837\n",
      "Training slice  3 ...\n",
      "Epoch 1\n",
      "Epoch: 1/1  Training Loss: 1.109  Test Loss: 0.487  Test Accuracy: 0.857\n",
      "Training slice  4 ...\n",
      "Epoch 1\n",
      "Epoch: 1/1  Training Loss: 1.252  Test Loss: 0.506  Test Accuracy: 0.860\n",
      "Training slice  5 ...\n",
      "Epoch 1\n",
      "Epoch: 1/1  Training Loss: 1.212  Test Loss: 0.507  Test Accuracy: 0.850\n",
      "Training slice  6 ...\n",
      "Epoch 1\n",
      "Epoch: 1/1  Training Loss: 1.119  Test Loss: 0.471  Test Accuracy: 0.870\n",
      "Training slice  7 ...\n",
      "Epoch 1\n",
      "Epoch: 1/1  Training Loss: 1.146  Test Loss: 0.501  Test Accuracy: 0.854\n",
      "Training slice  8 ...\n",
      "Epoch 1\n",
      "Epoch: 1/1  Training Loss: 1.146  Test Loss: 0.472  Test Accuracy: 0.864\n",
      "Training slice  9 ...\n",
      "Epoch 1\n",
      "Epoch: 1/1  Training Loss: 1.132  Test Loss: 0.511  Test Accuracy: 0.846\n",
      "Training slice  10 ...\n",
      "Epoch 1\n",
      "Epoch: 1/1  Training Loss: 1.204  Test Loss: 0.460  Test Accuracy: 0.863\n",
      "Training slice  11 ...\n",
      "Epoch 1\n",
      "Epoch: 1/1  Training Loss: 1.151  Test Loss: 0.497  Test Accuracy: 0.848\n",
      "Training slice  12 ...\n",
      "Epoch 1\n",
      "Epoch: 1/1  Training Loss: 1.196  Test Loss: 0.529  Test Accuracy: 0.835\n",
      "Training slice  13 ...\n",
      "Epoch 1\n",
      "Epoch: 1/1  Training Loss: 1.197  Test Loss: 0.552  Test Accuracy: 0.821\n",
      "Training slice  14 ...\n",
      "Epoch 1\n",
      "Epoch: 1/1  Training Loss: 1.216  Test Loss: 0.493  Test Accuracy: 0.861\n",
      "Training slice  15 ...\n",
      "Epoch 1\n",
      "Epoch: 1/1  Training Loss: 1.209  Test Loss: 0.522  Test Accuracy: 0.850\n",
      "Training slice  16 ...\n",
      "Epoch 1\n",
      "Epoch: 1/1  Training Loss: 1.169  Test Loss: 0.504  Test Accuracy: 0.823\n",
      "Training slice  17 ...\n",
      "Epoch 1\n",
      "Epoch: 1/1  Training Loss: 1.123  Test Loss: 0.474  Test Accuracy: 0.857\n",
      "Training slice  18 ...\n",
      "Epoch 1\n",
      "Epoch: 1/1  Training Loss: 1.107  Test Loss: 0.504  Test Accuracy: 0.846\n",
      "Training slice  19 ...\n",
      "Epoch 1\n",
      "Epoch: 1/1  Training Loss: 1.046  Test Loss: 0.517  Test Accuracy: 0.846\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import SubsetRandomSampler \n",
    "\n",
    "def get_trainloader_slice(i, slice_size):    \n",
    "    train_indices = range(int(i*slice_size), int((i+1)*slice_size))\n",
    "    trainloader = torch.utils.data.DataLoader(mnist_trainset, batch_size=64, shuffle=False, sampler=SubsetRandomSampler(train_indices))\n",
    "    return trainloader\n",
    "\n",
    "models = list()\n",
    "train_size = len(mnist_trainset)\n",
    "n_teachers = 20\n",
    "n_slices = n_teachers  #n_slices is deprecated\n",
    "slice_size = train_size / n_slices\n",
    "\n",
    "for i in range(n_slices):\n",
    "    print('Training slice ', i, '...')\n",
    "    new_model = Classifier()\n",
    "    new_trainloader = get_trainloader_slice(i, slice_size)\n",
    "    new_model.train_and_test(new_trainloader, testloader, 1)\n",
    "    models.append(new_model)    \n",
    "len(models)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 10000])\n"
     ]
    }
   ],
   "source": [
    "n_testset = len(mnist_testset)\n",
    "with torch.no_grad():    \n",
    "    private_label_matrix = torch.zeros(len(models), n_testset).long()\n",
    "    for i in range(len(models)):\n",
    "        model = models[i]\n",
    "        predictions = model.predict_dataset(testloader)        \n",
    "        private_label_matrix[i] = predictions\n",
    "    print(private_label_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def getLaplacianNoise(sensitivity, epsilon):\n",
    "    b = sensitivity / epsilon\n",
    "    return np.random.laplace(0, b, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 2, 1,  ..., 4, 5, 6])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp_labels = torch.zeros(n_testset).long()\n",
    "epsilon = 0.1\n",
    "sensitivity = 1\n",
    "nondp_labels = torch.zeros(n_testset).long()\n",
    "for i in range(n_testset):\n",
    "    private_labels = private_label_matrix[:,i].bincount(minlength=10)\n",
    "    \n",
    "    # print(private_labels.tolist())\n",
    "    noised_labels = list(map(lambda l: l + getLaplacianNoise(sensitivity, epsilon)[0], private_labels.tolist()))\n",
    "  \n",
    "    dp_labels[i] = int(np.argmax(noised_labels))\n",
    "    nondp_labels[i] = int(np.argmax(private_labels.tolist()))\n",
    "\n",
    "dp_labels\n",
    "nondp_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP model accuracy:  tensor(0.3969)\n"
     ]
    }
   ],
   "source": [
    "## Test accuracy\n",
    "equals = dp_labels == mnist_testset.targets\n",
    "accuracy = torch.mean(equals.type(torch.FloatTensor))    \n",
    "print('DP model accuracy: ', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edgarin\\.conda\\envs\\pysyft-env\\lib\\site-packages\\torchvision\\datasets\\mnist.py:58: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Epoch: 1/8  Training Loss: 2.096  Test Loss: 1.325  Test Accuracy: 0.811\n",
      "Epoch 2\n",
      "Epoch: 2/8  Training Loss: 2.008  Test Loss: 1.272  Test Accuracy: 0.840\n",
      "Epoch 3\n",
      "Epoch: 3/8  Training Loss: 1.985  Test Loss: 1.243  Test Accuracy: 0.836\n",
      "Epoch 4\n",
      "Epoch: 4/8  Training Loss: 1.973  Test Loss: 1.231  Test Accuracy: 0.838\n",
      "Epoch 5\n",
      "Epoch: 5/8  Training Loss: 1.960  Test Loss: 1.228  Test Accuracy: 0.831\n",
      "Epoch 6\n",
      "Epoch: 6/8  Training Loss: 1.941  Test Loss: 1.220  Test Accuracy: 0.827\n",
      "Epoch 7\n",
      "Epoch: 7/8  Training Loss: 1.930  Test Loss: 1.188  Test Accuracy: 0.827\n",
      "Epoch 8\n",
      "Epoch: 8/8  Training Loss: 1.913  Test Loss: 1.199  Test Accuracy: 0.818\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# mnist_testset.test_data.shape\n",
    "# torch.Size([10000, 28, 28])\n",
    "images = mnist_testset.test_data.float() / 255  # Needed for normalization\n",
    "\n",
    "dataset = TensorDataset(images, dp_labels.long())\n",
    "dploader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "dpmodel = Classifier()\n",
    "dpmodel.train_and_test(dploader, testloader, 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from syft.frameworks.torch.differential_privacy import pate\n",
    "\n",
    "data_dep_eps, data_ind_eps = pate.perform_analysis(\n",
    "    teacher_preds=private_label_matrix.numpy().astype(int), \n",
    "    indices=nondp_labels.numpy().astype(int), \n",
    "    noise_eps=0.001, \n",
    "    delta=1e-5,\n",
    "    moments=100\n",
    ")\n",
    "\n",
    "# assert data_dep_eps <= data_ind_eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Independent Epsilon: 0.9797052277070929\n",
      "Data Dependent Epsilon: 0.9797052277071728\n"
     ]
    }
   ],
   "source": [
    "print(\"Data Independent Epsilon:\", data_ind_eps)\n",
    "print(\"Data Dependent Epsilon:\", data_dep_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
