{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 6 \"Differential Privacy for Deep Learning\" project with MNIST (WIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note. This notebook is not final. The finished project without MNIST is in 'Section 3 - Project simple.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario description \n",
    "\n",
    "\n",
    "Training set: 60000 labeled images. This will be split in 20 slices of 3000 labeled images to generate the private models.\n",
    "Test set: 10000 images.\n",
    "The test set will have 2 purposes:\n",
    " - To help measure the training accuracy (using its labels for this)\n",
    " - To act as the \"public dataset\", by ignoring its labels. This is the main purpose of the testset\n",
    " \n",
    " Steps\n",
    "\n",
    "- 1) Train the 20 private models with the trainset\n",
    "- 2) Use the 20 partner models to predict on the local dataset (the testset), generating 20 labels for each of the datapoints\n",
    "- 3) Then, for each local data point (now with 20 labels), perform a maxcount query to get the most frequent label across the 20 labels. \n",
    "- 4) Add laplacian noise to the maxcounts to make this Differentially Private to a certain epsilon/delta constraint.\n",
    "- 5) Finally, we will retrain a new model on our local dataset which now has labels (the noised ones). This will be our final \"DP\" model.\n",
    "- 5) Do the PATE analysis to find the total epsilon used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project implementation\n",
    "\n",
    "This project will use the MNIST dataset, provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: ./data\n",
       "    Split: Test"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import torchvision.datasets as datasets\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "    ###transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "mnist_testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = mnist_trainset.data\n",
    "train_targets = mnist_trainset.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = mnist_testset.data\n",
    "test_targets = mnist_testset.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mnist_trainset.test_data.sum() == mnist_trainset.train_data.sum() True\n",
    "#mnist_testset.test_data.sum() == mnist_testset.train_data.sum() True\n",
    "#mnist_testset.test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 28, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mnist_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tf_encrypted:Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow (1.13.1). Fix this by compiling custom ops.\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "import syft as sy\n",
    "hook = sy.TorchHook(th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print('Device: ', torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "class MnistClassifier(nn.Module):\n",
    "    def __init__(self, worker = None):\n",
    "        super().__init__()       \n",
    "        \n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)                        \n",
    "        # Dropout module with 0.2 drop probability\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.criterion = nn.NLLLoss()\n",
    "        \n",
    "        if worker:\n",
    "            self.send(worker)\n",
    "            self.is_remote = True\n",
    "        else:\n",
    "            self.is_remote = False        \n",
    "            \n",
    "        print('is_remote', self.is_remote)    \n",
    "\n",
    "        # Use GPU if it's available\n",
    "        #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        #self.to(device);\n",
    "        \n",
    "    def get_device(self):\n",
    "        return \"cuda\" if next(self.parameters()).is_cuda else \"cpu\"\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Now with dropout\n",
    "        x = self.dropout(F.relu(self.fc1(x)))        \n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "\n",
    "        # output so no dropout here\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def process_batch(self, inputs, labels):\n",
    "        model = self\n",
    "        #print('  batch: ', batch_number)\n",
    "        # start = self.log_time_and_reset(start, \"    Get batch:\", profile)\n",
    "        # batch_number += 1                \n",
    "        # start = time.time() #Added by Edgarin\n",
    "        \n",
    "        # start = self.log_time_and_reset(start, \"    Move to device:\", profile)        \n",
    "\n",
    "        log_ps = model.forward(inputs)\n",
    "        # start = self.log_time_and_reset(start, \"    model.forward(): \", profile)\n",
    "        loss = self.criterion(log_ps, labels)\n",
    "        # print('loss.location: ', loss.location)\n",
    "        return (log_ps, loss)\n",
    "    \n",
    "    def train_dataset(self, trainloader):\n",
    "        model = self\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "        # print('model parameters: ', list(model.parameters()))\n",
    "        # print('model state_dict: ', model.state_dict())\n",
    "        train_loss = 0\n",
    "        for images, labels in trainloader:\n",
    "            print('batch')\n",
    "            if self.is_remote: images, labels = images.send(self.location), labels.send(self.location)\n",
    "            #images, labels = images.to(self.get_device()), labels.to(self.get_device()) # Added by Edgarin\n",
    "            #print('{')\n",
    "            #print('bobs objects: ', model.location._objects)\n",
    "            optimizer.zero_grad()\n",
    "            #print('}')\n",
    "            (log_ps, loss) = self.process_batch(images, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()            \n",
    "            # start = self.log_time_and_reset(start, \"    backward and step: \", profile)\n",
    "            train_loss += loss            \n",
    "        return (train_loss)\n",
    "    \n",
    "    def predict_dataset(self, testloader, isValidation = False, return_predictions = False):\n",
    "        model = self\n",
    "        test_loss = 0\n",
    "        accuracy = 0\n",
    "        \n",
    "        predictions = torch.tensor([]).long()\n",
    "\n",
    "        # Turn off gradients for validation, saves memory and computations                \n",
    "        with torch.no_grad():            \n",
    "            if isValidation: model.eval()  # To activate dropouts\n",
    "            for images, labels in testloader:\n",
    "                #start = time.time() #Added by Edgarin\n",
    "                if self.is_remote: images, labels = images.send(self.location), labels.send(self.location)\n",
    "                #images, labels = images.to(self.get_device()), labels.to(self.get_device()) # Added by Edgarin\n",
    "                \n",
    "                (log_ps, loss) = self.process_batch(images, labels)\n",
    "                \n",
    "                test_loss += loss\n",
    "\n",
    "                ps = torch.exp(log_ps)\n",
    "                \n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                equals = top_class == labels.view(*top_class.shape)\n",
    "                # accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "                print('batch accuracy: ', torch.mean(equals.float()).get())\n",
    "                accuracy += torch.mean(equals.float())\n",
    "                \n",
    "                #print(f\"Device = {self.get_device()}; Last test batch time: {(time.time() - start):.4f} seconds\") #Added by Edgarin\n",
    "                \n",
    "                if return_predictions:\n",
    "                    prediction = top_class.view(top_class.shape[0]).cpu()\n",
    "                    predictions = torch.cat((predictions, prediction), 0)\n",
    "            else:\n",
    "                model.train()        \n",
    "        return (accuracy, test_loss, predictions if return_predictions else None)\n",
    "    \n",
    "  \n",
    "    def log_time_and_reset(self, start, message = \"{:.4f} seconds\", profile = False):\n",
    "        new_start = time.time()\n",
    "        if profile: print((message + \" {:.0f} ms\").format((new_start - start)*1000)) #Added by Edgarin\n",
    "        return new_start\n",
    "    \n",
    "    \n",
    "    def train_and_test(self, trainloader, testloader, epochs = 1, profile = False):        \n",
    "        model = self                        \n",
    "\n",
    "        # epochs = 30  #Only thing changed from original\n",
    "        steps = 0\n",
    "\n",
    "        train_losses, test_losses = [], []\n",
    "        for e in range(epochs):\n",
    "            print('Epoch {}'.format(e+1))\n",
    "            b = 0\n",
    "            epoch_start = time.time()\n",
    "            start = time.time()\n",
    "            (train_loss) = self.train_dataset(trainloader)\n",
    "            train_loss = train_loss\n",
    "                        \n",
    "            #else:\n",
    "            if profile: print(f\"  Epoch time: {(time.time() - epoch_start):.4f} seconds\") #Added by Edgarin\n",
    "            if(False):                \n",
    "                ###\n",
    "                (accuracy, test_loss, _) = self.predict_dataset(testloader, isValidation = True)\n",
    "                accuracy = accuracy.get() ###Temp\n",
    "                test_loss = test_loss.get() ###Temp\n",
    "                \n",
    "                train_losses.append(train_loss/len(trainloader))\n",
    "                test_losses.append(test_loss/len(testloader))\n",
    "\n",
    "                print(\"Epoch: {}/{} \".format(e+1, epochs),\n",
    "                      \"Training Loss: {:.3f} \".format(train_loss/len(trainloader)),\n",
    "                      \"Test Loss: {:.3f} \".format(test_loss/len(testloader)),\n",
    "                      \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))\n",
    "                \n",
    "                #print(\"len(trainloader)\", len(trainloader))\n",
    "                #print(\"train_loss\", train_loss)\n",
    "                #print(\"train_loss/len(trainloader)\", train_loss/len(trainloader))\n",
    "                \n",
    "                \n",
    "                #print(\"Epoch: {}/{} \".format(e+1, epochs))\n",
    "                #print(\"Training Loss: {:.3f} \".format(train_loss/len(trainloader)))\n",
    "                #print(\"Test Loss: {:.3f} \".format(test_loss/len(testloader)))\n",
    "                #print(\"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))\n",
    "\n",
    "Classifier = MnistClassifier    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(mnist_trainset, batch_size=16384, shuffle=False)\n",
    "testloader = torch.utils.data.DataLoader(mnist_testset, batch_size=8192, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob = sy.VirtualWorker(hook, id=\"bob8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_remote True\n"
     ]
    }
   ],
   "source": [
    "bob.clear_objects()\n",
    "modelBob = Classifier(bob)\n",
    "#print(modelBob.location)\n",
    "#print('Bob objects before training: ', bob._objects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "Epoch 2\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "Epoch 3\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n"
     ]
    }
   ],
   "source": [
    "modelBob.train_and_test(trainloader, testloader, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{21591714816: Parameter containing:\n",
       " tensor([[-266645.6562, -266645.6250, -266645.6875,  ..., -266645.6875,\n",
       "          -266645.6562, -266645.6250],\n",
       "         [-266645.6250, -266645.6875, -266645.6875,  ..., -266645.6562,\n",
       "          -266645.6250, -266645.6250],\n",
       "         [-266645.6875, -266645.6875, -266645.6562,  ..., -266645.6875,\n",
       "          -266645.6562, -266645.6562],\n",
       "         ...,\n",
       "         [-266645.6875, -266645.6250, -266645.6250,  ..., -266645.6562,\n",
       "          -266645.6562, -266645.6875],\n",
       "         [-266645.6875, -266645.6250, -266645.6562,  ..., -266645.6562,\n",
       "          -266645.6875, -266645.6250],\n",
       "         [-266645.6250, -266645.6562, -266645.6250,  ..., -266645.6562,\n",
       "          -266645.6250, -266645.6250]], requires_grad=True),\n",
       " 9618960564: Parameter containing:\n",
       " tensor([-233148.0312, -234404.2812, -233314.3750, -233163.0938, -233141.8125,\n",
       "         -233177.4375, -233159.5938, -233151.5938, -233156.7812, -233152.0625,\n",
       "         -233178.0938, -233175.2188, -233134.6562, -233154.8125, -233147.3438,\n",
       "         -233143.3125, -233208.6250, -233141.9375, -233180.0938, -233163.6562,\n",
       "         -233176.1250, -233306.7500, -233190.0938, -233139.0625, -233137.3125,\n",
       "         -233143.2500, -233204.0938, -233142.9688, -233146.5000, -233137.6875,\n",
       "         -234575.6250, -233147.0625, -233150.0938, -233139.5000, -233179.0312,\n",
       "         -233174.0625, -233210.6875, -233145.3125, -233146.9688, -233142.4375,\n",
       "         -233141.9375, -233160.4062, -233173.0625, -233156.3438, -233144.9375,\n",
       "         -233300.8438, -233199.1875, -233162.1875, -233151.6875, -233392.1875,\n",
       "         -233231.8438, -233165.7500, -233137.9375, -233152.5312, -233151.1875,\n",
       "         -233186.7188, -233357.9062, -233161.8438, -233154.4062, -233174.3125,\n",
       "         -233141.5938, -233150.6250, -233158.1562, -233184.0625, -233254.5312,\n",
       "         -233136.7188, -233555.1875, -233391.3438, -233162.0938, -233459.6875,\n",
       "         -233137.8125, -233146.8750, -233208.2188, -233141.3125, -233141.3438,\n",
       "         -233161.2812, -233151.4062, -233142.0938, -233143.7500, -233143.0312,\n",
       "         -233312.5938, -234271.3438, -233146.6250, -233147.3438, -233148.7812,\n",
       "         -233178.7500, -233142.6875, -233191.1562, -233142.6562, -233139.6562,\n",
       "         -233147.0938, -233247.7812, -233153.0312, -233176.4688, -233263.7188,\n",
       "         -233143.1562, -233163.3438, -233144.0312, -233150.4688, -233176.2812,\n",
       "         -233158.9062, -233158.7812, -233173.0000, -233155.3438, -233137.6562,\n",
       "         -233175.5625, -233183.7812, -233147.3750, -233142.2812, -233178.1875,\n",
       "         -233163.3438, -233635.6250, -233159.0938, -233139.7188, -233187.3125,\n",
       "         -233149.6250, -233208.3750, -233166.9688, -233169.1562, -233158.1562,\n",
       "         -233175.4375, -233142.1875, -236913.2500, -233308.2188, -233314.1250,\n",
       "         -233145.4062, -233196.7812, -233205.7812], requires_grad=True),\n",
       " 72783545765: Parameter containing:\n",
       " tensor([[-234147.9375, -278442.0000, -233643.0312,  ..., -233680.1562,\n",
       "          -233230.6250, -233309.3438],\n",
       "         [-233486.3125, -291413.9375, -234856.3125,  ..., -233715.7188,\n",
       "          -236813.3438, -234324.0312],\n",
       "         [-233220.3125, -238238.6562, -234772.6562,  ..., -233175.8750,\n",
       "          -233170.5938, -233271.7812],\n",
       "         ...,\n",
       "         [-233428.7500, -263445.6875, -235004.5312,  ..., -233607.3438,\n",
       "          -233900.8438, -234709.3125],\n",
       "         [-233246.0625, -280809.1250, -234309.0312,  ..., -233235.2812,\n",
       "          -233245.7812, -233229.4688],\n",
       "         [-233597.0938, -259680.9062, -235891.6562,  ..., -233431.2188,\n",
       "          -233431.8125, -233327.2812]], requires_grad=True),\n",
       " 47612311850: Parameter containing:\n",
       " tensor([-233138.1562, -233272.7500, -233154.9375, -233133.2500, -233134.5625,\n",
       "         -233146.2188, -233273.4062, -233261.3125, -233151.3750, -233189.6875,\n",
       "         -233147.0938, -233135.8750, -233146.3438, -233196.0625, -233136.5625,\n",
       "         -233143.9688, -233136.0938, -233138.0312, -233149.3125, -233135.2500,\n",
       "         -233154.2500, -233219.5938, -233137.2500, -233143.1875, -233134.5625,\n",
       "         -233201.4062, -233147.0625, -233139.0938, -233150.6562, -233150.2812,\n",
       "         -233312.0625, -233135.3125, -233133.4688, -233151.5625, -233175.7500,\n",
       "         -233142.7500, -233133.8438, -233151.7812, -233136.6562, -234701.9688,\n",
       "         -233143.3750, -233160.7812, -233165.0625, -233140.9688, -233135.9375,\n",
       "         -233139.7500, -233141.8750, -233142.6250, -233135.0938, -233149.6875,\n",
       "         -233139.5312, -233134.6250, -233138.9062, -233132.5312, -233224.6250,\n",
       "         -233137.3438, -233137.8125, -233158.3750, -233143.9062, -233139.3750,\n",
       "         -233141.0000, -233174.5625, -233160.6875, -233154.0625],\n",
       "        requires_grad=True),\n",
       " 78324637687: Parameter containing:\n",
       " tensor([[-233264.3125, -233201.2188, -233149.5625, -233139.3438, -233177.9375,\n",
       "          -233140.1250, -233140.0938, -233135.5938, -233165.5312, -233149.0938,\n",
       "          -233204.9062, -233310.8750, -233166.2188, -234082.6250, -233219.2500,\n",
       "          -233144.6250, -233180.0625, -233147.4375, -233268.0625, -233175.5000,\n",
       "          -233161.6562, -233142.7812, -233148.2500, -233137.7500, -233180.5625,\n",
       "          -233191.8125, -233140.1562, -233156.4688, -233173.5312, -233368.8125,\n",
       "          -258163.1562, -233138.6250, -233136.0938, -233279.8125, -233139.0938,\n",
       "          -233185.6875, -233135.8438, -233604.4062, -235730.3750, -233159.5312,\n",
       "          -233143.0312, -233143.1562, -233167.6250, -233146.0000, -233209.7188,\n",
       "          -233136.1875, -233399.0938, -233154.2812, -233138.0000, -233454.1562,\n",
       "          -233565.9375, -233134.9062, -233146.6250, -233136.4688, -233142.8125,\n",
       "          -233257.6562, -233162.3750, -233209.0625, -233140.0312, -233171.8125,\n",
       "          -233554.5938, -233257.6562, -233165.4375, -233477.4688],\n",
       "         [-233171.6562, -233192.8438, -233142.0938, -233189.5938, -233164.6250,\n",
       "          -233149.0312, -233139.2500, -233135.4688, -233156.0625, -233141.3750,\n",
       "          -233151.2188, -233158.1562, -233490.6250, -233821.1875, -233156.8125,\n",
       "          -233228.6875, -233145.5000, -233134.7812, -233182.6875, -233377.0000,\n",
       "          -233188.7812, -233153.3125, -233147.9688, -233149.7812, -233140.4062,\n",
       "          -233142.1562, -233157.6875, -233187.4688, -233154.7812, -233293.5938,\n",
       "          -234711.7812, -233283.4062, -233138.6562, -233146.4062, -233140.5625,\n",
       "          -233369.8750, -233134.4688, -233150.3125, -233161.8438, -233145.3438,\n",
       "          -233157.8125, -233150.2812, -233160.0312, -233140.2500, -233149.6250,\n",
       "          -233142.7500, -233276.1562, -233138.7188, -233137.3438, -233229.7500,\n",
       "          -233147.8750, -233135.2812, -233141.5312, -233134.7812, -233160.1562,\n",
       "          -233184.4375, -235043.4062, -233570.2500, -233146.1875, -233147.5000,\n",
       "          -233198.9375, -233269.5000, -233167.1562, -233613.9062],\n",
       "         [-233174.7500, -233217.3438, -233142.1562, -233141.6250, -233138.0312,\n",
       "          -233147.0938, -233231.5000, -234463.5938, -233818.1562, -233142.4688,\n",
       "          -233225.7812, -233177.5625, -233512.1562, -233807.7188, -233147.7812,\n",
       "          -233158.6562, -233139.8438, -233144.5938, -233201.5625, -233192.1250,\n",
       "          -233294.8438, -233305.9375, -233138.4375, -237503.5000, -233139.3438,\n",
       "          -233158.0938, -233159.4062, -233498.1562, -233278.1562, -233400.0000,\n",
       "          -241958.2188, -233212.5625, -233149.0625, -233156.0000, -233138.7812,\n",
       "          -233151.0625, -233147.5938, -233254.6875, -233242.0312, -233146.5312,\n",
       "          -233179.9688, -233145.5938, -233265.3125, -233168.2500, -233152.1250,\n",
       "          -233173.1875, -233212.5000, -233157.7188, -233145.4375, -233874.8750,\n",
       "          -233145.6875, -233170.7500, -233178.2188, -233145.5000, -233158.5625,\n",
       "          -233159.4688, -233150.4688, -233252.5000, -233145.3438, -233190.1250,\n",
       "          -233450.5938, -233248.1250, -233180.8125, -233563.9375],\n",
       "         [-233344.0312, -233316.5000, -233152.0625, -233145.1250, -233137.5312,\n",
       "          -233140.8125, -233143.6875, -233151.1250, -233204.1562, -233208.6875,\n",
       "          -233162.9688, -233167.1250, -233330.4375, -233825.3750, -233147.7188,\n",
       "          -233610.2188, -233140.4688, -233137.0625, -233773.4688, -233634.5312,\n",
       "          -233218.5312, -239857.6250, -233138.8750, -233138.1562, -233145.9062,\n",
       "          -233207.5000, -233204.7188, -235047.1562, -233180.6250, -233380.3438,\n",
       "          -234588.1250, -233148.7188, -233142.6562, -233137.4062, -233157.1875,\n",
       "          -233148.8438, -233161.9688, -234658.3125, -233184.3125, -233301.6562,\n",
       "          -233159.5938, -233140.2188, -233435.7500, -233155.6875, -233151.9375,\n",
       "          -233137.3750, -233166.6875, -233254.1875, -233140.6250, -233936.1250,\n",
       "          -233140.4688, -233136.1250, -233140.5625, -233143.5938, -233147.4062,\n",
       "          -233137.6875, -233141.4688, -233278.7500, -233154.5938, -233147.9062,\n",
       "          -258082.1875, -233413.8750, -233227.1562, -233378.1875],\n",
       "         [-233390.0312, -234661.3750, -233169.1875, -233145.2500, -233135.0312,\n",
       "          -233143.0938, -233141.8438, -233198.5625, -233306.4688, -233152.3750,\n",
       "          -233884.1875, -234648.6875, -233322.0938, -234091.0938, -233152.5312,\n",
       "          -233155.9375, -233139.5312, -233156.2812, -233275.7188, -233202.1875,\n",
       "          -233263.7188, -233201.8438, -233138.0625, -233147.6562, -233147.4062,\n",
       "          -233221.3750, -233158.9688, -233186.5000, -233245.6562, -233304.8438,\n",
       "          -238819.9062, -233152.0000, -233221.1250, -233142.3750, -233137.4375,\n",
       "          -233217.6562, -233173.8750, -233209.3125, -233183.5312, -233179.2188,\n",
       "          -233336.8750, -233169.0312, -233158.1250, -233304.5000, -233689.3438,\n",
       "          -233158.6562, -233238.5938, -233164.1562, -233155.5312, -234452.2500,\n",
       "          -233159.6562, -233199.6250, -237292.4062, -233170.6562, -233189.2500,\n",
       "          -233139.2500, -233182.1562, -234032.8125, -233160.6562, -233258.3750,\n",
       "          -233446.1562, -234193.3125, -233176.1562, -234522.8438],\n",
       "         [-233216.9062, -233267.5625, -233149.5312, -233260.0000, -233148.9062,\n",
       "          -233160.9062, -233143.8750, -233157.0312, -233163.5938, -233149.9688,\n",
       "          -233186.6562, -233182.6562, -233569.1562, -271567.8750, -233155.9062,\n",
       "          -233239.3750, -233155.6875, -233160.1250, -233315.9375, -233210.8125,\n",
       "          -233227.8438, -233212.9375, -233168.7812, -233256.6250, -233180.7500,\n",
       "          -233165.3125, -233164.0938, -233281.9062, -233161.3438, -233312.1875,\n",
       "          -235538.7812, -233181.8438, -233140.4688, -233196.0625, -233146.0625,\n",
       "          -233179.1250, -233397.8438, -233193.3125, -233218.4688, -233175.1250,\n",
       "          -233185.5000, -233149.0938, -233327.0000, -234007.7188, -233145.0938,\n",
       "          -233238.7812, -233261.1562, -233148.4062, -233167.3438, -234120.0000,\n",
       "          -233210.6250, -233141.8750, -233170.5938, -233232.4688, -233199.1562,\n",
       "          -233181.8125, -233189.3438, -233201.0000, -233156.6250, -233326.5000,\n",
       "          -234285.1250, -233379.1875, -233750.2188, -236654.0625],\n",
       "         [-233195.8125, -233222.0938, -233152.2500, -233256.0625, -233135.8125,\n",
       "          -233155.9688, -233149.9375, -233151.6875, -233236.0000, -233154.5938,\n",
       "          -233800.8750, -233259.1250, -233295.9375, -272771.0625, -233146.6875,\n",
       "          -233158.3438, -233165.2188, -233140.5312, -233230.1250, -233380.6875,\n",
       "          -233160.3750, -233184.4062, -233178.5312, -233152.2812, -233278.0938,\n",
       "          -233143.7500, -233145.7500, -233180.1875, -233169.9375, -233243.7500,\n",
       "          -235559.3750, -233151.4062, -233137.4062, -233201.1562, -233137.0938,\n",
       "          -233168.0312, -233140.1562, -233294.8125, -233216.5938, -233142.5625,\n",
       "          -233159.4375, -233137.9688, -233167.0625, -233162.7188, -233156.0938,\n",
       "          -233152.1562, -233145.6875, -233141.5000, -233140.0312, -233441.0000,\n",
       "          -233136.0938, -233144.9375, -233149.0000, -233147.2812, -233159.5938,\n",
       "          -233141.2812, -234390.2500, -234716.9375, -233145.0625, -233141.7188,\n",
       "          -233445.0938, -233239.0938, -233462.2500, -233444.7188],\n",
       "         [-233286.0625, -233192.9062, -233172.0625, -233141.5625, -233139.6875,\n",
       "          -233171.8750, -233142.5000, -233144.5312, -233184.7188, -233166.0938,\n",
       "          -233174.8438, -233182.8750, -233682.1250, -234288.3125, -233183.3750,\n",
       "          -233149.3438, -233166.4375, -233139.7188, -233339.7500, -233151.8125,\n",
       "          -233195.7812, -233160.5938, -233200.8750, -233139.7500, -233160.9062,\n",
       "          -233142.7188, -233143.7500, -233184.7188, -233154.8750, -233162.1250,\n",
       "          -234448.6562, -233140.9375, -233134.4375, -233144.0312, -233152.0625,\n",
       "          -233174.5938, -233230.3125, -233183.4375, -233216.6562, -233144.2500,\n",
       "          -233167.5312, -233139.0625, -233154.4688, -233149.8438, -233189.4062,\n",
       "          -233137.5938, -233141.3750, -233246.0625, -233139.4375, -233763.2500,\n",
       "          -233138.5938, -233135.5312, -233145.6250, -233152.3125, -233147.7500,\n",
       "          -233182.0938, -233139.1562, -233233.3750, -233172.5938, -233147.4375,\n",
       "          -237175.2812, -234382.1875, -233184.8750, -233634.1250],\n",
       "         [-233190.7812, -233229.4062, -233147.0000, -233137.5938, -233141.0312,\n",
       "          -233152.6562, -233193.5625, -233170.0938, -233160.2188, -233158.5312,\n",
       "          -233317.6875, -233317.1562, -233426.1875, -256767.9062, -234110.4375,\n",
       "          -233147.8438, -233148.0000, -233146.0000, -233283.6562, -233177.6250,\n",
       "          -233241.6875, -233209.5938, -233160.8438, -233139.6875, -233163.2812,\n",
       "          -233148.6562, -233151.9062, -234885.9375, -233460.2188, -233392.5000,\n",
       "          -234376.3750, -233155.5625, -233142.2812, -233177.5625, -233160.9062,\n",
       "          -233223.6250, -233156.0000, -233260.0312, -233317.0625, -233155.3438,\n",
       "          -233157.2812, -233175.0625, -233178.7500, -233465.4062, -233161.7812,\n",
       "          -233335.3750, -233144.9688, -233208.5938, -233221.6250, -233553.4688,\n",
       "          -233164.7812, -233165.1875, -233162.5312, -233147.2812, -233276.1875,\n",
       "          -233140.8125, -233141.7812, -233285.5312, -234421.6562, -233212.2812,\n",
       "          -234039.0312, -233462.4375, -233186.1250, -235399.5312],\n",
       "         [-233205.4688, -233205.9688, -233177.6250, -233152.9062, -234123.0625,\n",
       "          -233154.3438, -233152.2500, -233394.8438, -233198.0000, -233174.1250,\n",
       "          -233405.8125, -233166.2500, -233333.7188, -233415.3125, -233170.9688,\n",
       "          -233157.7500, -234092.3125, -233163.7188, -233373.8438, -233173.0312,\n",
       "          -233226.4375, -233260.2500, -233148.3750, -233142.2500, -233163.0000,\n",
       "          -233154.3750, -233163.3438, -233194.9062, -233149.4375, -233554.5625,\n",
       "          -234382.8125, -233145.0000, -233137.9375, -233148.5000, -233148.2500,\n",
       "          -233206.5000, -233143.5625, -233193.5312, -233654.4062, -233169.5000,\n",
       "          -233153.7812, -233136.9688, -233360.1562, -233189.2188, -233434.3438,\n",
       "          -233156.8125, -233142.4062, -233160.9062, -233136.8125, -235300.3125,\n",
       "          -233149.0938, -233137.9688, -233138.3750, -233139.9062, -233153.9062,\n",
       "          -233179.5625, -233137.8125, -233392.2500, -233197.5000, -233155.0000,\n",
       "          -234377.6562, -233707.3125, -233183.5000, -233333.2500]],\n",
       "        requires_grad=True),\n",
       " 71259873366: Parameter containing:\n",
       " tensor([ -0.8380,  -1.3561,  -2.9787,  -2.5391,  -2.4940, -10.0221,  -0.5760,\n",
       "          -2.4614,  -2.6245,  -2.5550], requires_grad=True),\n",
       " 47183398908: tensor(11.7210, grad_fn=<AddBackward0>),\n",
       " 19462011262: tensor(6.2013),\n",
       " 29075604078: tensor(0.1954),\n",
       " 83704793695: tensor(12.5582, grad_fn=<AddBackward0>),\n",
       " 61317730742: tensor(6.2609),\n",
       " 87730017382: tensor(0.1954),\n",
       " 51058258915: tensor(12.6751, grad_fn=<AddBackward0>),\n",
       " 83382080499: tensor(6.3265),\n",
       " 42221328013: tensor(0.1954),\n",
       " 24727361246: tensor(12.8441, grad_fn=<AddBackward0>),\n",
       " 3885832674: tensor(6.4338),\n",
       " 69132058149: tensor(0.1954),\n",
       " 22690341718: tensor(13.0512, grad_fn=<AddBackward0>),\n",
       " 570373930: tensor(6.5348),\n",
       " 8106786996: tensor(0.1954)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import mnist_classifier\n",
    "#Classifier = mnist_classifier.MnistClassifier\n",
    "bob._objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- cpu ---\n",
      "is_remote True\n",
      "Epoch 1\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch accuracy:  tensor(0.0903)\n",
      "batch accuracy:  tensor(0.0841)\n",
      "Epoch: 1/5  Training Loss: 2.645  Test Loss: 2.780  Test Accuracy: 0.087\n",
      "Epoch 2\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch accuracy:  tensor(0.0903)\n",
      "batch accuracy:  tensor(0.0841)\n",
      "Epoch: 2/5  Training Loss: 2.785  Test Loss: 2.805  Test Accuracy: 0.087\n",
      "Epoch 3\n"
     ]
    }
   ],
   "source": [
    "for device in ['cpu']:  \n",
    "    print('---', device, '---')\n",
    "    ## Test training a model\n",
    "    start = time.time()\n",
    "    modelBob = Classifier(bob)\n",
    "    #modelBob.to(device)    \n",
    "    modelBob.train_and_test(trainloader, testloader, 5)\n",
    "    print(f\"Device = {modelBob.get_device()}; Whole time: {(time.time() - start):.4f} seconds\") #Added by Edgarin\n",
    "print('done')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import helper\n",
    "\n",
    "images, labels = next(iter(testloader))\n",
    "img = images[0] #.view(1, 784)\n",
    "if modelBob.is_remote: img = img.send(bob)\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    #logps = modelBob.forward(img.to(model1.get_device())).cpu()\n",
    "    logps = modelBob.forward(img)\n",
    "    print(torch.exp(logps).sum().get())\n",
    "\n",
    "# Output of the network are logits, need to take softmax for probabilities\n",
    "ps = torch.exp(logps)\n",
    "if modelBob.is_remote: \n",
    "    helper.view_classify(img.get().view(1, 28, 28), ps.get())\n",
    "else:\n",
    "    helper.view_classify(img.view(1, 28, 28), ps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import SubsetRandomSampler \n",
    "\n",
    "def get_trainloader_slice(i, slice_size):    \n",
    "    train_indices = range(int(i*slice_size), int((i+1)*slice_size))\n",
    "    trainloader = torch.utils.data.DataLoader(mnist_trainset, batch_size=512, shuffle=False, sampler=SubsetRandomSampler(train_indices))\n",
    "    return trainloader\n",
    "\n",
    "models = list()\n",
    "train_size = len(mnist_trainset)\n",
    "n_teachers = 200\n",
    "n_slices = n_teachers  #n_slices is deprecated\n",
    "slice_size = train_size / n_slices\n",
    "\n",
    "for i in range(n_slices):\n",
    "    print('----- Training slice ', i, '... -----')\n",
    "    new_model = Classifier()\n",
    "    # print('is_cuda', next(new_model.parameters()).is_cuda)\n",
    "    new_trainloader = get_trainloader_slice(i, slice_size)\n",
    "    new_model.train_and_test(new_trainloader, testloader, 3)\n",
    "    models.append(new_model)    \n",
    "len(models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_testset = len(mnist_testset)\n",
    "with torch.no_grad():    \n",
    "    private_label_matrix = torch.zeros(len(models), n_testset).long()\n",
    "    for i in range(len(models)):\n",
    "        model = models[i]\n",
    "        # predictions = model.predict_dataset(testloader)        \n",
    "        predictions = predict_dataset(model, testloader)        \n",
    "        private_label_matrix[i] = predictions\n",
    "    print(private_label_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def getLaplacianNoise(sensitivity, epsilon):\n",
    "    b = sensitivity / epsilon\n",
    "    return np.random.laplace(0, b, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_labels = torch.zeros(n_testset).long()\n",
    "epsilon = 0.1\n",
    "sensitivity = 1\n",
    "nondp_labels = torch.zeros(n_testset).long()\n",
    "for i in range(n_testset):\n",
    "    private_labels = private_label_matrix[:,i].bincount(minlength=10)\n",
    "    \n",
    "    # print(private_labels.tolist())\n",
    "    noised_labels = list(map(lambda l: l + getLaplacianNoise(sensitivity, epsilon)[0], private_labels.tolist()))\n",
    "  \n",
    "    dp_labels[i] = int(np.argmax(noised_labels))\n",
    "    nondp_labels[i] = int(np.argmax(private_labels.tolist()))\n",
    "\n",
    "dp_labels\n",
    "nondp_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test accuracy\n",
    "equals = dp_labels == mnist_testset.targets\n",
    "accuracy = torch.mean(equals.type(torch.FloatTensor))    \n",
    "print('DP model accuracy: ', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# mnist_testset.test_data.shape\n",
    "# torch.Size([10000, 28, 28])\n",
    "images = mnist_testset.data.float() / 255  # Needed for normalization\n",
    "\n",
    "dataset = TensorDataset(images, dp_labels.long())\n",
    "dploader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "dpmodel = Classifier()\n",
    "dpmodel.train_and_test(dploader, testloader, 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from syft.frameworks.torch.differential_privacy import pate\n",
    "\n",
    "data_dep_eps, data_ind_eps = pate.perform_analysis(\n",
    "    teacher_preds=private_label_matrix.numpy().astype(int), \n",
    "    indices=nondp_labels.numpy().astype(int), \n",
    "    noise_eps=0.1, \n",
    "    delta=1e-5,\n",
    "    #moments=100\n",
    ")\n",
    "\n",
    "# assert data_dep_eps <= data_ind_eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data Independent Epsilon:\", data_ind_eps)\n",
    "print(\"Data Dependent Epsilon:\", data_dep_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
