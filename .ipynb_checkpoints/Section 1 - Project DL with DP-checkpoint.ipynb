{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 6 \"Differential Privacy for Deep Learning\" project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario description (TODO)\n",
    "\n",
    "First we're going to consider a scenario - you work for a hospital and you have a large collection of images about your patients. However, you don't know what's in them. You would like to use these images to develop a neural network which can automatically classify them, however since your images aren't labeled, they aren't sufficient to train a classifier. \n",
    "\n",
    "However, being a cunning strategist, you realize that you can reach out to 10 partner hospitals which DO have annotated data. It is your hope to train your new classifier on their datasets so that you can automatically label your own. While these hospitals are interested in helping, they have privacy concerns regarding information about their patients. Thus, you will use the following technique to train a classifier which protects the privacy of patients in the other hospitals.\n",
    "\n",
    "- 1) You'll ask each of the 10 hospitals to train a model on their own datasets (All of which have the same kinds of labels)\n",
    "- 2) You'll then use each of the 10 partner models to predict on your local dataset, generating 10 labels for each of your datapoints\n",
    "- 3) Then, for each local data point (now with 10 labels), you will perform a DP query to generate the final true label. This query is a \"max\" function, where \"max\" is the most frequent label across the 10 labels. We will need to add laplacian noise to make this Differentially Private to a certain epsilon/delta constraint.\n",
    "- 4) Finally, we will retrain a new model on our local dataset which now has labels. This will be our final \"DP\" model.\n",
    "\n",
    "So, let's walk through these steps. I will assume you're already familiar with how to train/predict a deep neural network, so we'll skip steps 1 and 2 and work with example data. We'll focus instead on step 3, namely how to perform the DP query for each example using toy data.\n",
    "\n",
    "So, let's say we have 10,000 training examples, and we've got 10 labels for each example (from our 10 \"teacher models\" which were trained directly on private data). Each label is chosen from a set of 10 possible labels (categories) for each image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section Project:\n",
    "\n",
    "For the final project for this section, you're going to train a DP model using this PATE method on the MNIST dataset, provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: ./data\n",
       "    Split: Test"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import torchvision.datasets as datasets\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "    #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "mnist_testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edgarin\\.conda\\envs\\pysyft-env\\lib\\site-packages\\torchvision\\datasets\\mnist.py:53: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n",
      "C:\\Users\\edgarin\\.conda\\envs\\pysyft-env\\lib\\site-packages\\torchvision\\datasets\\mnist.py:43: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "train_data = mnist_trainset.train_data\n",
    "train_targets = mnist_trainset.train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edgarin\\.conda\\envs\\pysyft-env\\lib\\site-packages\\torchvision\\datasets\\mnist.py:58: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "C:\\Users\\edgarin\\.conda\\envs\\pysyft-env\\lib\\site-packages\\torchvision\\datasets\\mnist.py:48: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "test_data = mnist_testset.test_data\n",
    "test_targets = mnist_testset.test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mnist_trainset.test_data.sum() == mnist_trainset.train_data.sum() True\n",
    "#mnist_testset.test_data.sum() == mnist_testset.train_data.sum() True\n",
    "#mnist_testset.test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 28, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mnist_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "        # Dropout module with 0.2 drop probability\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Now with dropout\n",
    "        x = self.dropout(F.relu(self.fc1(x)))        \n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "\n",
    "        # output so no dropout here\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def predict_dataset(self, dataloader):\n",
    "        model = self\n",
    "        #predictions = list()\n",
    "        predictions = torch.tensor([]).long()\n",
    "        with torch.no_grad():\n",
    "            for images, _ in dataloader:                \n",
    "                log_ps = model(images)                \n",
    "                ps = torch.exp(log_ps)\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                prediction = top_class.view(top_class.shape[0])\n",
    "                #print('prediction.shape', prediction.shape)\n",
    "                predictions = torch.cat((predictions, prediction), 0)\n",
    "                #predictions.append(prediction)\n",
    "                \n",
    "        return predictions\n",
    "            \n",
    "    def train_and_test(self, trainloader, testloader, epochs = 1):\n",
    "        # Copied from Part 5 - Inference and Validation (Solution).ipynb\n",
    "        model = self\n",
    "        \n",
    "        criterion = nn.NLLLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "        # epochs = 30  #Only thing changed from original\n",
    "        steps = 0\n",
    "\n",
    "        train_losses, test_losses = [], []\n",
    "        for e in range(epochs):\n",
    "            print('Epoch {}'.format(e+1))\n",
    "            running_loss = 0\n",
    "            for images, labels in trainloader:\n",
    "                # images = images.view(images.shape[0], -1)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                log_ps = model(images)\n",
    "                loss = criterion(log_ps, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            else:\n",
    "                test_loss = 0\n",
    "                accuracy = 0\n",
    "                \n",
    "                # Turn off gradients for validation, saves memory and computations                \n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    for images, labels in testloader:\n",
    "                        # images = images.view(images.shape[0], -1)\n",
    "                        log_ps = model(images)\n",
    "                        test_loss += criterion(log_ps, labels)\n",
    "\n",
    "                        ps = torch.exp(log_ps)\n",
    "                        top_p, top_class = ps.topk(1, dim=1)\n",
    "                        equals = top_class == labels.view(*top_class.shape)\n",
    "                        accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "\n",
    "                model.train()\n",
    "\n",
    "                train_losses.append(running_loss/len(trainloader))\n",
    "                test_losses.append(test_loss/len(testloader))\n",
    "\n",
    "                print(\"Epoch: {}/{} \".format(e+1, epochs),\n",
    "                      \"Training Loss: {:.3f} \".format(running_loss/len(trainloader)),\n",
    "                      \"Test Loss: {:.3f} \".format(test_loss/len(testloader)),\n",
    "                      \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(mnist_trainset, batch_size=64, shuffle=False)\n",
    "testloader = torch.utils.data.DataLoader(mnist_testset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Epoch: 1/1  Training Loss: 0.334  Test Loss: 0.161  Test Accuracy: 0.951\n"
     ]
    }
   ],
   "source": [
    "## Test training a model\n",
    "model1 = Classifier()\n",
    "model1.train_and_test(trainloader, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import helper\n",
    "\n",
    "images, labels = next(iter(testloader))\n",
    "img = images[0] #.view(1, 784)\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logps = model1.forward(img)\n",
    "\n",
    "# Output of the network are logits, need to take softmax for probabilities\n",
    "ps = torch.exp(logps)\n",
    "helper.view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training slice  0 ...\n",
      "Epoch 1\n",
      "Epoch: 1/8  Training Loss: 1.117  Test Loss: 0.541  Test Accuracy: 0.844\n",
      "Epoch 2\n",
      "Epoch: 2/8  Training Loss: 0.470  Test Loss: 0.384  Test Accuracy: 0.884\n",
      "Epoch 3\n",
      "Epoch: 3/8  Training Loss: 0.341  Test Loss: 0.344  Test Accuracy: 0.898\n",
      "Epoch 4\n",
      "Epoch: 4/8  Training Loss: 0.278  Test Loss: 0.310  Test Accuracy: 0.905\n",
      "Epoch 5\n",
      "Epoch: 5/8  Training Loss: 0.218  Test Loss: 0.287  Test Accuracy: 0.914\n",
      "Epoch 6\n",
      "Epoch: 6/8  Training Loss: 0.176  Test Loss: 0.288  Test Accuracy: 0.917\n",
      "Epoch 7\n",
      "Epoch: 7/8  Training Loss: 0.141  Test Loss: 0.269  Test Accuracy: 0.923\n",
      "Epoch 8\n",
      "Epoch: 8/8  Training Loss: 0.114  Test Loss: 0.299  Test Accuracy: 0.916\n",
      "Training slice  1 ...\n",
      "Epoch 1\n",
      "Epoch: 1/8  Training Loss: 1.137  Test Loss: 0.480  Test Accuracy: 0.864\n",
      "Epoch 2\n",
      "Epoch: 2/8  Training Loss: 0.412  Test Loss: 0.373  Test Accuracy: 0.889\n",
      "Epoch 3\n",
      "Epoch: 3/8  Training Loss: 0.298  Test Loss: 0.332  Test Accuracy: 0.900\n",
      "Epoch 4\n",
      "Epoch: 4/8  Training Loss: 0.232  Test Loss: 0.294  Test Accuracy: 0.911\n",
      "Epoch 5\n",
      "Epoch: 5/8  Training Loss: 0.190  Test Loss: 0.287  Test Accuracy: 0.913\n",
      "Epoch 6\n",
      "Epoch: 6/8  Training Loss: 0.156  Test Loss: 0.291  Test Accuracy: 0.915\n",
      "Epoch 7\n",
      "Epoch: 7/8  Training Loss: 0.121  Test Loss: 0.294  Test Accuracy: 0.916\n",
      "Epoch 8\n",
      "Epoch: 8/8  Training Loss: 0.120  Test Loss: 0.261  Test Accuracy: 0.925\n",
      "Training slice  2 ...\n",
      "Epoch 1\n",
      "Epoch: 1/8  Training Loss: 1.153  Test Loss: 0.486  Test Accuracy: 0.859\n",
      "Epoch 2\n",
      "Epoch: 2/8  Training Loss: 0.471  Test Loss: 0.367  Test Accuracy: 0.891\n",
      "Epoch 3\n",
      "Epoch: 3/8  Training Loss: 0.329  Test Loss: 0.327  Test Accuracy: 0.902\n",
      "Epoch 4\n",
      "Epoch: 4/8  Training Loss: 0.259  Test Loss: 0.306  Test Accuracy: 0.908\n",
      "Epoch 5\n",
      "Epoch: 5/8  Training Loss: 0.202  Test Loss: 0.281  Test Accuracy: 0.919\n",
      "Epoch 6\n",
      "Epoch: 6/8  Training Loss: 0.186  Test Loss: 0.299  Test Accuracy: 0.912\n",
      "Epoch 7\n",
      "Epoch: 7/8  Training Loss: 0.151  Test Loss: 0.316  Test Accuracy: 0.914\n",
      "Epoch 8\n",
      "Epoch: 8/8  Training Loss: 0.120  Test Loss: 0.280  Test Accuracy: 0.923\n",
      "Training slice  3 ...\n",
      "Epoch 1\n",
      "Epoch: 1/8  Training Loss: 1.080  Test Loss: 0.494  Test Accuracy: 0.853\n",
      "Epoch 2\n",
      "Epoch: 2/8  Training Loss: 0.404  Test Loss: 0.365  Test Accuracy: 0.891\n",
      "Epoch 3\n",
      "Epoch: 3/8  Training Loss: 0.285  Test Loss: 0.339  Test Accuracy: 0.895\n",
      "Epoch 4\n",
      "Epoch: 4/8  Training Loss: 0.223  Test Loss: 0.321  Test Accuracy: 0.902\n",
      "Epoch 5\n",
      "Epoch: 5/8  Training Loss: 0.178  Test Loss: 0.304  Test Accuracy: 0.909\n",
      "Epoch 6\n",
      "Epoch: 6/8  Training Loss: 0.140  Test Loss: 0.272  Test Accuracy: 0.918\n",
      "Epoch 7\n",
      "Epoch: 7/8  Training Loss: 0.122  Test Loss: 0.274  Test Accuracy: 0.919\n",
      "Epoch 8\n",
      "Epoch: 8/8  Training Loss: 0.091  Test Loss: 0.299  Test Accuracy: 0.919\n",
      "Training slice  4 ...\n",
      "Epoch 1\n",
      "Epoch: 1/8  Training Loss: 1.185  Test Loss: 0.502  Test Accuracy: 0.849\n",
      "Epoch 2\n",
      "Epoch: 2/8  Training Loss: 0.498  Test Loss: 0.359  Test Accuracy: 0.898\n",
      "Epoch 3\n",
      "Epoch: 3/8  Training Loss: 0.355  Test Loss: 0.305  Test Accuracy: 0.904\n",
      "Epoch 4\n",
      "Epoch: 4/8  Training Loss: 0.287  Test Loss: 0.297  Test Accuracy: 0.912\n",
      "Epoch 5\n",
      "Epoch: 5/8  Training Loss: 0.225  Test Loss: 0.244  Test Accuracy: 0.927\n",
      "Epoch 6\n",
      "Epoch: 6/8  Training Loss: 0.173  Test Loss: 0.258  Test Accuracy: 0.922\n",
      "Epoch 7\n",
      "Epoch: 7/8  Training Loss: 0.167  Test Loss: 0.255  Test Accuracy: 0.924\n",
      "Epoch 8\n",
      "Epoch: 8/8  Training Loss: 0.128  Test Loss: 0.229  Test Accuracy: 0.934\n",
      "Training slice  5 ...\n",
      "Epoch 1\n",
      "Epoch: 1/8  Training Loss: 1.198  Test Loss: 0.526  Test Accuracy: 0.837\n",
      "Epoch 2\n",
      "Epoch: 2/8  Training Loss: 0.471  Test Loss: 0.358  Test Accuracy: 0.896\n",
      "Epoch 3\n",
      "Epoch: 3/8  Training Loss: 0.340  Test Loss: 0.303  Test Accuracy: 0.911\n",
      "Epoch 4\n",
      "Epoch: 4/8  Training Loss: 0.274  Test Loss: 0.263  Test Accuracy: 0.921\n",
      "Epoch 5\n",
      "Epoch: 5/8  Training Loss: 0.209  Test Loss: 0.260  Test Accuracy: 0.924\n",
      "Epoch 6\n",
      "Epoch: 6/8  Training Loss: 0.174  Test Loss: 0.255  Test Accuracy: 0.928\n",
      "Epoch 7\n",
      "Epoch: 7/8  Training Loss: 0.150  Test Loss: 0.278  Test Accuracy: 0.925\n",
      "Epoch 8\n",
      "Epoch: 8/8  Training Loss: 0.115  Test Loss: 0.257  Test Accuracy: 0.933\n",
      "Training slice  6 ...\n",
      "Epoch 1\n",
      "Epoch: 1/8  Training Loss: 1.176  Test Loss: 0.484  Test Accuracy: 0.861\n",
      "Epoch 2\n",
      "Epoch: 2/8  Training Loss: 0.453  Test Loss: 0.362  Test Accuracy: 0.891\n",
      "Epoch 3\n",
      "Epoch: 3/8  Training Loss: 0.315  Test Loss: 0.352  Test Accuracy: 0.897\n",
      "Epoch 4\n",
      "Epoch: 4/8  Training Loss: 0.256  Test Loss: 0.299  Test Accuracy: 0.907\n",
      "Epoch 5\n",
      "Epoch: 5/8  Training Loss: 0.190  Test Loss: 0.292  Test Accuracy: 0.911\n",
      "Epoch 6\n",
      "Epoch: 6/8  Training Loss: 0.167  Test Loss: 0.312  Test Accuracy: 0.907\n",
      "Epoch 7\n",
      "Epoch: 7/8  Training Loss: 0.138  Test Loss: 0.278  Test Accuracy: 0.918\n",
      "Epoch 8\n",
      "Epoch: 8/8  Training Loss: 0.114  Test Loss: 0.305  Test Accuracy: 0.908\n",
      "Training slice  7 ...\n",
      "Epoch 1\n",
      "Epoch: 1/8  Training Loss: 1.171  Test Loss: 0.505  Test Accuracy: 0.866\n",
      "Epoch 2\n",
      "Epoch: 2/8  Training Loss: 0.460  Test Loss: 0.361  Test Accuracy: 0.895\n",
      "Epoch 3\n",
      "Epoch: 3/8  Training Loss: 0.321  Test Loss: 0.334  Test Accuracy: 0.900\n",
      "Epoch 4\n",
      "Epoch: 4/8  Training Loss: 0.245  Test Loss: 0.293  Test Accuracy: 0.913\n",
      "Epoch 5\n",
      "Epoch: 5/8  Training Loss: 0.207  Test Loss: 0.296  Test Accuracy: 0.915\n",
      "Epoch 6\n",
      "Epoch: 6/8  Training Loss: 0.181  Test Loss: 0.293  Test Accuracy: 0.910\n",
      "Epoch 7\n",
      "Epoch: 7/8  Training Loss: 0.131  Test Loss: 0.288  Test Accuracy: 0.918\n",
      "Epoch 8\n",
      "Epoch: 8/8  Training Loss: 0.118  Test Loss: 0.276  Test Accuracy: 0.923\n",
      "Training slice  8 ...\n",
      "Epoch 1\n",
      "Epoch: 1/8  Training Loss: 1.147  Test Loss: 0.510  Test Accuracy: 0.853\n",
      "Epoch 2\n",
      "Epoch: 2/8  Training Loss: 0.476  Test Loss: 0.383  Test Accuracy: 0.885\n",
      "Epoch 3\n",
      "Epoch: 3/8  Training Loss: 0.344  Test Loss: 0.324  Test Accuracy: 0.905\n",
      "Epoch 4\n",
      "Epoch: 4/8  Training Loss: 0.248  Test Loss: 0.295  Test Accuracy: 0.914\n",
      "Epoch 5\n",
      "Epoch: 5/8  Training Loss: 0.214  Test Loss: 0.296  Test Accuracy: 0.916\n",
      "Epoch 6\n",
      "Epoch: 6/8  Training Loss: 0.168  Test Loss: 0.282  Test Accuracy: 0.917\n",
      "Epoch 7\n",
      "Epoch: 7/8  Training Loss: 0.155  Test Loss: 0.349  Test Accuracy: 0.901\n",
      "Epoch 8\n",
      "Epoch: 8/8  Training Loss: 0.128  Test Loss: 0.273  Test Accuracy: 0.923\n",
      "Training slice  9 ...\n",
      "Epoch 1\n",
      "Epoch: 1/8  Training Loss: 1.163  Test Loss: 0.522  Test Accuracy: 0.845\n",
      "Epoch 2\n",
      "Epoch: 2/8  Training Loss: 0.475  Test Loss: 0.355  Test Accuracy: 0.891\n",
      "Epoch 3\n",
      "Epoch: 3/8  Training Loss: 0.353  Test Loss: 0.298  Test Accuracy: 0.911\n",
      "Epoch 4\n",
      "Epoch: 4/8  Training Loss: 0.270  Test Loss: 0.283  Test Accuracy: 0.914\n",
      "Epoch 5\n",
      "Epoch: 5/8  Training Loss: 0.219  Test Loss: 0.275  Test Accuracy: 0.917\n",
      "Epoch 6\n",
      "Epoch: 6/8  Training Loss: 0.195  Test Loss: 0.266  Test Accuracy: 0.919\n",
      "Epoch 7\n",
      "Epoch: 7/8  Training Loss: 0.143  Test Loss: 0.266  Test Accuracy: 0.923\n",
      "Epoch 8\n",
      "Epoch: 8/8  Training Loss: 0.111  Test Loss: 0.280  Test Accuracy: 0.921\n",
      "Training slice  10 ...\n",
      "Epoch 1\n",
      "Epoch: 1/8  Training Loss: 1.224  Test Loss: 0.474  Test Accuracy: 0.862\n",
      "Epoch 2\n",
      "Epoch: 2/8  Training Loss: 0.515  Test Loss: 0.360  Test Accuracy: 0.892\n",
      "Epoch 3\n",
      "Epoch: 3/8  Training Loss: 0.366  Test Loss: 0.304  Test Accuracy: 0.908\n",
      "Epoch 4\n",
      "Epoch: 4/8  Training Loss: 0.272  Test Loss: 0.291  Test Accuracy: 0.911\n",
      "Epoch 5\n",
      "Epoch: 5/8  Training Loss: 0.231  Test Loss: 0.272  Test Accuracy: 0.919\n",
      "Epoch 6\n",
      "Epoch: 6/8  Training Loss: 0.184  Test Loss: 0.267  Test Accuracy: 0.920\n",
      "Epoch 7\n",
      "Epoch: 7/8  Training Loss: 0.150  Test Loss: 0.265  Test Accuracy: 0.923\n",
      "Epoch 8\n",
      "Epoch: 8/8  Training Loss: 0.113  Test Loss: 0.249  Test Accuracy: 0.929\n",
      "Training slice  11 ...\n",
      "Epoch 1\n",
      "Epoch: 1/8  Training Loss: 1.173  Test Loss: 0.511  Test Accuracy: 0.850\n",
      "Epoch 2\n",
      "Epoch: 2/8  Training Loss: 0.436  Test Loss: 0.369  Test Accuracy: 0.890\n",
      "Epoch 3\n",
      "Epoch: 3/8  Training Loss: 0.317  Test Loss: 0.355  Test Accuracy: 0.898\n",
      "Epoch 4\n",
      "Epoch: 4/8  Training Loss: 0.241  Test Loss: 0.311  Test Accuracy: 0.907\n",
      "Epoch 5\n",
      "Epoch: 5/8  Training Loss: 0.187  Test Loss: 0.283  Test Accuracy: 0.918\n",
      "Epoch 6\n",
      "Epoch: 6/8  Training Loss: 0.161  Test Loss: 0.292  Test Accuracy: 0.916\n",
      "Epoch 7\n",
      "Epoch: 7/8  Training Loss: 0.137  Test Loss: 0.287  Test Accuracy: 0.919\n",
      "Epoch 8\n",
      "Epoch: 8/8  Training Loss: 0.112  Test Loss: 0.290  Test Accuracy: 0.921\n",
      "Training slice  12 ...\n",
      "Epoch 1\n",
      "Epoch: 1/8  Training Loss: 1.191  Test Loss: 0.534  Test Accuracy: 0.844\n",
      "Epoch 2\n",
      "Epoch: 2/8  Training Loss: 0.483  Test Loss: 0.370  Test Accuracy: 0.887\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/8  Training Loss: 0.354  Test Loss: 0.328  Test Accuracy: 0.901\n",
      "Epoch 4\n",
      "Epoch: 4/8  Training Loss: 0.274  Test Loss: 0.311  Test Accuracy: 0.905\n",
      "Epoch 5\n",
      "Epoch: 5/8  Training Loss: 0.223  Test Loss: 0.289  Test Accuracy: 0.912\n",
      "Epoch 6\n",
      "Epoch: 6/8  Training Loss: 0.180  Test Loss: 0.277  Test Accuracy: 0.918\n",
      "Epoch 7\n",
      "Epoch: 7/8  Training Loss: 0.138  Test Loss: 0.238  Test Accuracy: 0.930\n",
      "Epoch 8\n",
      "Epoch: 8/8  Training Loss: 0.124  Test Loss: 0.262  Test Accuracy: 0.923\n",
      "Training slice  13 ...\n",
      "Epoch 1\n",
      "Epoch: 1/8  Training Loss: 1.225  Test Loss: 0.556  Test Accuracy: 0.827\n",
      "Epoch 2\n",
      "Epoch: 2/8  Training Loss: 0.516  Test Loss: 0.410  Test Accuracy: 0.871\n",
      "Epoch 3\n",
      "Epoch: 3/8  Training Loss: 0.367  Test Loss: 0.329  Test Accuracy: 0.901\n",
      "Epoch 4\n",
      "Epoch: 4/8  Training Loss: 0.278  Test Loss: 0.320  Test Accuracy: 0.905\n",
      "Epoch 5\n",
      "Epoch: 5/8  Training Loss: 0.230  Test Loss: 0.310  Test Accuracy: 0.906\n",
      "Epoch 6\n",
      "Epoch: 6/8  Training Loss: 0.191  Test Loss: 0.277  Test Accuracy: 0.916\n",
      "Epoch 7\n",
      "Epoch: 7/8  Training Loss: 0.167  Test Loss: 0.293  Test Accuracy: 0.916\n",
      "Epoch 8\n",
      "Epoch: 8/8  Training Loss: 0.147  Test Loss: 0.291  Test Accuracy: 0.916\n",
      "Training slice  14 ...\n",
      "Epoch 1\n",
      "Epoch: 1/8  Training Loss: 1.158  Test Loss: 0.482  Test Accuracy: 0.859\n",
      "Epoch 2\n",
      "Epoch: 2/8  Training Loss: 0.472  Test Loss: 0.362  Test Accuracy: 0.885\n",
      "Epoch 3\n",
      "Epoch: 3/8  Training Loss: 0.328  Test Loss: 0.336  Test Accuracy: 0.894\n",
      "Epoch 4\n",
      "Epoch: 4/8  Training Loss: 0.288  Test Loss: 0.346  Test Accuracy: 0.893\n",
      "Epoch 5\n",
      "Epoch: 5/8  Training Loss: 0.228  Test Loss: 0.290  Test Accuracy: 0.915\n",
      "Epoch 6\n",
      "Epoch: 6/8  Training Loss: 0.171  Test Loss: 0.295  Test Accuracy: 0.917\n",
      "Epoch 7\n",
      "Epoch: 7/8  Training Loss: 0.145  Test Loss: 0.266  Test Accuracy: 0.926\n",
      "Epoch 8\n",
      "Epoch: 8/8  Training Loss: 0.128  Test Loss: 0.275  Test Accuracy: 0.923\n",
      "Training slice  15 ...\n",
      "Epoch 1\n",
      "Epoch: 1/8  Training Loss: 1.194  Test Loss: 0.538  Test Accuracy: 0.826\n",
      "Epoch 2\n",
      "Epoch: 2/8  Training Loss: 0.511  Test Loss: 0.406  Test Accuracy: 0.878\n",
      "Epoch 3\n",
      "Epoch: 3/8  Training Loss: 0.379  Test Loss: 0.331  Test Accuracy: 0.906\n",
      "Epoch 4\n",
      "Epoch: 4/8  Training Loss: 0.299  Test Loss: 0.296  Test Accuracy: 0.912\n",
      "Epoch 5\n",
      "Epoch: 5/8  Training Loss: 0.243  Test Loss: 0.281  Test Accuracy: 0.917\n",
      "Epoch 6\n",
      "Epoch: 6/8  Training Loss: 0.201  Test Loss: 0.275  Test Accuracy: 0.918\n",
      "Epoch 7\n",
      "Epoch: 7/8  Training Loss: 0.173  Test Loss: 0.272  Test Accuracy: 0.921\n",
      "Epoch 8\n",
      "Epoch: 8/8  Training Loss: 0.132  Test Loss: 0.287  Test Accuracy: 0.921\n",
      "Training slice  16 ...\n",
      "Epoch 1\n",
      "Epoch: 1/8  Training Loss: 1.181  Test Loss: 0.498  Test Accuracy: 0.839\n",
      "Epoch 2\n",
      "Epoch: 2/8  Training Loss: 0.471  Test Loss: 0.343  Test Accuracy: 0.897\n",
      "Epoch 3\n",
      "Epoch: 3/8  Training Loss: 0.338  Test Loss: 0.301  Test Accuracy: 0.906\n",
      "Epoch 4\n",
      "Epoch: 4/8  Training Loss: 0.253  Test Loss: 0.274  Test Accuracy: 0.916\n",
      "Epoch 5\n",
      "Epoch: 5/8  Training Loss: 0.222  Test Loss: 0.304  Test Accuracy: 0.907\n",
      "Epoch 6\n",
      "Epoch: 6/8  Training Loss: 0.182  Test Loss: 0.316  Test Accuracy: 0.903\n",
      "Epoch 7\n",
      "Epoch: 7/8  Training Loss: 0.155  Test Loss: 0.302  Test Accuracy: 0.910\n",
      "Epoch 8\n",
      "Epoch: 8/8  Training Loss: 0.131  Test Loss: 0.261  Test Accuracy: 0.927\n",
      "Training slice  17 ...\n",
      "Epoch 1\n",
      "Epoch: 1/8  Training Loss: 1.117  Test Loss: 0.490  Test Accuracy: 0.846\n",
      "Epoch 2\n",
      "Epoch: 2/8  Training Loss: 0.450  Test Loss: 0.369  Test Accuracy: 0.887\n",
      "Epoch 3\n",
      "Epoch: 3/8  Training Loss: 0.339  Test Loss: 0.346  Test Accuracy: 0.895\n",
      "Epoch 4\n",
      "Epoch: 4/8  Training Loss: 0.272  Test Loss: 0.334  Test Accuracy: 0.899\n",
      "Epoch 5\n",
      "Epoch: 5/8  Training Loss: 0.204  Test Loss: 0.277  Test Accuracy: 0.916\n",
      "Epoch 6\n",
      "Epoch: 6/8  Training Loss: 0.170  Test Loss: 0.290  Test Accuracy: 0.910\n",
      "Epoch 7\n",
      "Epoch: 7/8  Training Loss: 0.145  Test Loss: 0.272  Test Accuracy: 0.916\n",
      "Epoch 8\n",
      "Epoch: 8/8  Training Loss: 0.108  Test Loss: 0.259  Test Accuracy: 0.922\n",
      "Training slice  18 ...\n",
      "Epoch 1\n",
      "Epoch: 1/8  Training Loss: 1.097  Test Loss: 0.466  Test Accuracy: 0.864\n",
      "Epoch 2\n",
      "Epoch: 2/8  Training Loss: 0.434  Test Loss: 0.378  Test Accuracy: 0.888\n",
      "Epoch 3\n",
      "Epoch: 3/8  Training Loss: 0.313  Test Loss: 0.320  Test Accuracy: 0.902\n",
      "Epoch 4\n",
      "Epoch: 4/8  Training Loss: 0.233  Test Loss: 0.329  Test Accuracy: 0.897\n",
      "Epoch 5\n",
      "Epoch: 5/8  Training Loss: 0.181  Test Loss: 0.275  Test Accuracy: 0.917\n",
      "Epoch 6\n",
      "Epoch: 6/8  Training Loss: 0.152  Test Loss: 0.293  Test Accuracy: 0.912\n",
      "Epoch 7\n",
      "Epoch: 7/8  Training Loss: 0.129  Test Loss: 0.264  Test Accuracy: 0.923\n",
      "Epoch 8\n",
      "Epoch: 8/8  Training Loss: 0.116  Test Loss: 0.279  Test Accuracy: 0.921\n",
      "Training slice  19 ...\n",
      "Epoch 1\n",
      "Epoch: 1/8  Training Loss: 1.075  Test Loss: 0.516  Test Accuracy: 0.838\n",
      "Epoch 2\n",
      "Epoch: 2/8  Training Loss: 0.366  Test Loss: 0.395  Test Accuracy: 0.878\n",
      "Epoch 3\n",
      "Epoch: 3/8  Training Loss: 0.235  Test Loss: 0.391  Test Accuracy: 0.881\n",
      "Epoch 4\n",
      "Epoch: 4/8  Training Loss: 0.179  Test Loss: 0.353  Test Accuracy: 0.901\n",
      "Epoch 5\n",
      "Epoch: 5/8  Training Loss: 0.132  Test Loss: 0.344  Test Accuracy: 0.900\n",
      "Epoch 6\n",
      "Epoch: 6/8  Training Loss: 0.113  Test Loss: 0.336  Test Accuracy: 0.906\n",
      "Epoch 7\n",
      "Epoch: 7/8  Training Loss: 0.088  Test Loss: 0.370  Test Accuracy: 0.900\n",
      "Epoch 8\n",
      "Epoch: 8/8  Training Loss: 0.067  Test Loss: 0.320  Test Accuracy: 0.915\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.utils.data\n",
    "# from torch.utils.data import SubsetRandomSampler \n",
    "\n",
    "def get_trainloader_slice(i, slice_size):    \n",
    "    train_indices = range(int(i*slice_size), int((i+1)*slice_size))\n",
    "    trainloader = torch.utils.data.DataLoader(mnist_trainset, batch_size=64, shuffle=False, sampler=torch.utils.data.SubsetRandomSampler(train_indices))\n",
    "    return trainloader\n",
    "\n",
    "models = list()\n",
    "train_size = len(mnist_trainset)\n",
    "n_teachers = 20\n",
    "n_slices = n_teachers  #n_slices is deprecated\n",
    "slice_size = train_size / n_slices\n",
    "\n",
    "for i in range(n_slices):\n",
    "    print('Training slice ', i, '...')\n",
    "    new_model = Classifier()\n",
    "    new_trainloader = get_trainloader_slice(i, slice_size)\n",
    "    new_model.train_and_test(new_trainloader, testloader, 8)\n",
    "    models.append(new_model)    \n",
    "len(models)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 10000])\n"
     ]
    }
   ],
   "source": [
    "n_testset = len(mnist_testset)\n",
    "with torch.no_grad():    \n",
    "    private_label_matrix = torch.zeros(len(models), n_testset).long()\n",
    "    for i in range(len(models)):\n",
    "        model = models[i]\n",
    "        predictions = model.predict_dataset(testloader)        \n",
    "        private_label_matrix[i] = predictions\n",
    "    print(private_label_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def getLaplacianNoise(sensitivity, epsilon):\n",
    "    b = sensitivity / epsilon\n",
    "    return np.random.laplace(0, b, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 2, 1,  ..., 4, 5, 6])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp_labels = torch.zeros(n_testset).long()\n",
    "epsilon = 0.1\n",
    "sensitivity = 1\n",
    "nondp_labels = torch.zeros(n_testset).long()\n",
    "for i in range(n_testset):\n",
    "    private_labels = private_label_matrix[:,i].bincount(minlength=10)\n",
    "    \n",
    "    # print(private_labels.tolist())\n",
    "    noised_labels = list(map(lambda l: l + getLaplacianNoise(sensitivity, epsilon)[0], private_labels.tolist()))\n",
    "  \n",
    "    dp_labels[i] = int(np.argmax(noised_labels))\n",
    "    nondp_labels[i] = int(np.argmax(private_labels.tolist()))\n",
    "\n",
    "dp_labels\n",
    "nondp_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dp_labels.dtype:  torch.int64\n",
      "test_labels.dtype torch.int64\n",
      "tensor(0.4545)\n"
     ]
    }
   ],
   "source": [
    "## Test accuracy\n",
    "equals = dp_labels == mnist_testset.targets\n",
    "accuracy = torch.mean(equals.type(torch.FloatTensor))    \n",
    "print('DP model accuracy: ', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Epoch: 1/8  Training Loss: 2.061  Test Loss: 1.193  Test Accuracy: 0.815\n",
      "Epoch 2\n",
      "Epoch: 2/8  Training Loss: 1.961  Test Loss: 1.134  Test Accuracy: 0.869\n",
      "Epoch 3\n",
      "Epoch: 3/8  Training Loss: 1.932  Test Loss: 1.130  Test Accuracy: 0.877\n",
      "Epoch 4\n",
      "Epoch: 4/8  Training Loss: 1.909  Test Loss: 1.124  Test Accuracy: 0.864\n",
      "Epoch 5\n",
      "Epoch: 5/8  Training Loss: 1.892  Test Loss: 1.094  Test Accuracy: 0.877\n",
      "Epoch 6\n",
      "Epoch: 6/8  Training Loss: 1.872  Test Loss: 1.135  Test Accuracy: 0.873\n",
      "Epoch 7\n",
      "Epoch: 7/8  Training Loss: 1.863  Test Loss: 1.149  Test Accuracy: 0.867\n",
      "Epoch 8\n",
      "Epoch: 8/8  Training Loss: 1.830  Test Loss: 1.170  Test Accuracy: 0.859\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# mnist_testset.test_data.shape\n",
    "# torch.Size([10000, 28, 28])\n",
    "\n",
    "# images = mnist_testset.test_data.float().view(10000, 1, 28, 28) / 255\n",
    "images = mnist_testset.test_data.float() / 255\n",
    "\n",
    "dataset = TensorDataset(images, dp_labels.long())\n",
    "dploader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "dpmodel = Classifier()\n",
    "dpmodel.train_and_test(dploader, testloader, 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from syft.frameworks.torch.differential_privacy import pate\n",
    "\n",
    "data_dep_eps, data_ind_eps = pate.perform_analysis(\n",
    "    teacher_preds=private_label_matrix, \n",
    "    indices=nondp_labels, \n",
    "    noise_eps=0.1, \n",
    "    delta=1e-5\n",
    ")\n",
    "\n",
    "assert data_dep_eps < data_ind_eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data Independent Epsilon:\", data_ind_eps)\n",
    "print(\"Data Dependent Epsilon:\", data_dep_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import helper\n",
    "\n",
    "images, labels = next(iter(dploader))\n",
    "print('images.shape', images.shape)\n",
    "print('labels.shape', labels.shape)\n",
    "\n",
    "print('images.dtype', images.dtype)\n",
    "print('labels.dtype', labels.dtype)\n",
    "#images = images.view(64, 1, 28, 28)\n",
    "img = images[0] #.view(1, 784)\n",
    "\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logps = model1.forward(img)\n",
    "\n",
    "# Output of the network are logits, need to take softmax for probabilities\n",
    "ps = torch.exp(logps)\n",
    "helper.view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timages, _ = next(iter(testloader))\n",
    "dpimages, _ = next(iter(dploader))\n",
    "index = 0\n",
    "#dpimages[index] = transforms.Normalize((0.5,), (0.5, ))(dpimages[index])\n",
    "dpimages[index]/= 255\n",
    "print(timages[index].shape, dpimages[index].shape)\n",
    "print(timages[index].dtype, dpimages[index].dtype)\n",
    "print(timages[index].sum(), dpimages[index].sum())\n",
    "print(timages[index, 0])\n",
    "print(dpimages[index, 0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "inps = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\n",
    "tgts = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\n",
    "print(inps)\n",
    "print(tgts)\n",
    "dataset = TensorDataset(inps, tgts)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=4)\n",
    "\n",
    "for batch_ndx, sample in enumerate(loader):\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = torch.zeros([3, 4])\n",
    "#print(r)\n",
    "# r = torch.cat((r, x), 0)\n",
    "# r = torch.cat((r, y), 0)\n",
    "r[0] = x\n",
    "r[1] = y\n",
    "r[2] = z\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = r.int()\n",
    "r[:,0].bincount(minlength=10)\n",
    "\n",
    "#x.bincount(minlength = 10)\n",
    "#rt = r.transpose(0, 1).int()\n",
    "#print(rt)\n",
    "#rt.bincount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
